{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 512\n",
    "\n",
    "#giving the training and testing data path\n",
    "train_data_dir = './nxp/train/'\n",
    "validation_data_dir = './nxp/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the image data using image data generator\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "#we can use either the code above for image data generation or the below code for generation\n",
    "#Also for generating the image data using image data generator(we can specify how many image we want to convert using one image)\n",
    "#(using parameter like rotation range,shear range,zoom rangr,flip etc)\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator( \n",
    "        rotation_range = 40, \n",
    "        shear_range = 0.4, \n",
    "        zoom_range = 0.3, \n",
    "        horizontal_flip = True, \n",
    "        brightness_range = (0.5, 1.5)) \n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "#path = \"/home/bluebrain/Dynamic_Advertisement/nxp/train/\"\n",
    "a = [\"Angry\",\"Disgust\",\"Happy\",\"Neutral\",\"Fear\",\"Sad\",\"Surprise\"]\n",
    "for i in a:\n",
    "    dirs = os.listdir(\"/home/bluebrain/Dynamic_Advertisement/nxp/train/\"+i)\n",
    "    path = \"/home/bluebrain/Dynamic_Advertisement/nxp/train/\"+i\n",
    "    for item in dirs:\n",
    "        if item == \".DS_STORE\":\n",
    "            continue\n",
    "        if os.path.isfile(path+item):\n",
    "            img = Image.open(path+item)\n",
    "            x = img_to_array(img) \n",
    "            x = x.reshape(x.shape)   \n",
    "# using the above defined parameters.  \n",
    "            j = 0\n",
    "            for batch in datagen.flow(x, batch_size = 1, \n",
    "                          save_to_dir ='/home/bluebrain/Dynamic_Advertisement/nxp/train/'+i,  \n",
    "                          save_prefix ='image', save_format ='jpeg'):\n",
    "                j += 1\n",
    "                if j > 4:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove checkpoints remove using(moving to the directory showing the checkpoints and remove by using the command\n",
    "#\"rm -r .ipynb_checkpoints\"\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture of model used\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.0001),input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(7, kernel_size=(1, 1), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Conv2D(7, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 2227\n",
    "nb_validation_samples = 408\n",
    "epochs = 100\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "model.save('model.h656')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(\"model.h55\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model('model.h55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model(running it on youtube video)\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import pafy\n",
    "url = \"https://www.youtube.com/watch?v=tiicx0d7yBg&t=20s\"\n",
    "my_video = pafy.new(url)\n",
    "play = my_video.getbest(preftype = \"mp4\")\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(play.url)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    ch = cv2.waitKey(1)\n",
    "    if ch == 27 or ch == ord('q') or ch == ord('Q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
